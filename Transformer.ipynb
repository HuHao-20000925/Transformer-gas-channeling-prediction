{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dbab9d0-23a4-41cb-a9d4-e01409ff5d2c",
   "metadata": {},
   "source": [
    "# 一、数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f40697-16bd-4024-b509-78262c716496",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from PIL import Image\n",
    "from natsort import natsorted\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from tqdm import tqdm\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# 假设 time_steps 已经定义\n",
    "time_steps = 6\n",
    "image_size = 128\n",
    "\n",
    "# 1.1 固定数值数据\n",
    "\n",
    "def read_numeric_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df[['grid_length', 'grid_thickness', 'depth', 'temperature', 'pressure']].values\n",
    "\n",
    "\n",
    "file_path = r'3000数值型数据.csv'\n",
    "fixed_numeric_data = read_numeric_data(file_path)\n",
    "\n",
    "\n",
    "for i in range(fixed_numeric_data.shape[1]):\n",
    "    scaler_fixed = MinMaxScaler()\n",
    "    column_data = fixed_numeric_data[:, i].reshape(-1, 1)\n",
    "    column_data = scaler_fixed.fit_transform(column_data)\n",
    "    fixed_numeric_data[:, i] = column_data.flatten()\n",
    "\n",
    "# 将数据从 [0, 1] 转换到 [-1, 1]\n",
    "fixed_numeric_data = torch.from_numpy(fixed_numeric_data).float()  # 关键修改\n",
    "fixed_numeric_data = fixed_numeric_data * 2 - 1\n",
    "\n",
    "print(\"固定数据特征:\", fixed_numeric_data.shape)\n",
    "print(\"前六行特征值:\")\n",
    "print(fixed_numeric_data[:6, :])\n",
    "\n",
    "# 1.2读取注入量数据\n",
    "def read_injection_data(folder_path):\n",
    "    data_files = natsorted(os.listdir(folder_path))\n",
    "    all_data = []\n",
    "    print(f\"正在读取前缘数据，文件顺序如下：\")\n",
    "    for file in data_files:\n",
    "        if not file.endswith('.txt'):\n",
    "            continue\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        print(file_path)\n",
    "        try:\n",
    "            with open(file_path, 'r') as f:\n",
    "                content = f.read().strip().split()\n",
    "                data = [float(x) for x in content]\n",
    "                if len(data) != time_steps:\n",
    "                    print(f\"文件 {file_path} 中的数据数量不是 {time_steps}，请检查。\")\n",
    "                    continue\n",
    "                all_data.append(np.array(data))\n",
    "        except Exception as e:\n",
    "            print(f\"读取文件 {file_path} 时出现问题: {e}\")\n",
    "    all_data = np.array(all_data)\n",
    "\n",
    "    # 打印前五行数据\n",
    "    if all_data.size > 0:\n",
    "        print(\"读取到的数据的前五行如下：\")\n",
    "        print(all_data[:5])\n",
    "\n",
    "    return all_data\n",
    "\n",
    "# 1.3读取压力数据\n",
    "def read_pressure_data(folder_path):\n",
    "    data_files = natsorted(os.listdir(folder_path))\n",
    "    all_data = []\n",
    "    print(f\"正在读取压力数据，文件顺序如下：\")\n",
    "    for file in data_files:\n",
    "        if not file.endswith('.txt'):\n",
    "            continue\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        print(file_path)\n",
    "        try:\n",
    "            with open(file_path, 'r') as f:\n",
    "                content = f.read().strip().split()\n",
    "                data = [float(x) for x in content]\n",
    "                if len(data) != time_steps:\n",
    "                    print(f\"文件 {file_path} 中的数据数量不是 {time_steps}，请检查。\")\n",
    "                    continue\n",
    "                all_data.append(np.array(data))\n",
    "        except Exception as e:\n",
    "            print(f\"读取文件 {file_path} 时出现问题: {e}\")\n",
    "    return np.array(all_data)\n",
    "\n",
    "# 1.4读取饱和度变异系数数据\n",
    "def read_saturation_variation_data(folder_path):\n",
    "    data_files = natsorted(os.listdir(folder_path))\n",
    "    all_data = []\n",
    "    success_count = 0\n",
    "    skip_count = 0\n",
    "    print(f\"正在读取饱和度变异系数数据，文件顺序如下：\")\n",
    "    for file in data_files:\n",
    "        if not file.endswith('.txt'):\n",
    "            continue\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        print(file_path)\n",
    "        try:\n",
    "            with open(file_path, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "                data = []\n",
    "                for line in lines:\n",
    "                    line = line.strip()\n",
    "                    if '%' in line:\n",
    "                        try:\n",
    "                            line = line.replace('%', '').strip()\n",
    "                            line = float(line) / 100.0\n",
    "                        except ValueError:\n",
    "                            print(f\"无法将 {line} 转换为有效的百分比数据，请检查文件 {file_path}。\")\n",
    "                            continue\n",
    "                    else:\n",
    "                        try:\n",
    "                            line = float(line)\n",
    "                        except ValueError:\n",
    "                            print(f\"无法将 {line} 转换为有效的数值，请检查文件 {file_path}。\")\n",
    "                            continue\n",
    "                    data.append(line)\n",
    "                if len(data) != time_steps:\n",
    "                    print(f\"文件 {file_path} 中的数据数量不是 {time_steps}，请检查。\")\n",
    "                    skip_count += 1\n",
    "                    continue\n",
    "                all_data.append(np.array(data))\n",
    "                success_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"读取文件 {file_path} 时出现问题: {e}\")\n",
    "            skip_count += 1\n",
    "    print(f\"成功读取 {success_count} 个文件，跳过 {skip_count} 个文件。\")\n",
    "    all_data = np.array(all_data)\n",
    "\n",
    "    # 打印前五行数据\n",
    "    if all_data.size > 0:\n",
    "        print(\"读取到的数据的前五行如下：\")\n",
    "        print(all_data[:5])\n",
    "\n",
    "    return all_data\n",
    "\n",
    "\n",
    "injection_data = read_injection_data(r'3000注入量')\n",
    "pressure_data = read_pressure_data(r'3000压力')\n",
    "saturation_data = read_saturation_variation_data(r'3000饱和度变异系数')\n",
    "\n",
    "\n",
    "# 按所有样本的同一特征在所有时间步上进行归一化\n",
    "def normalize_all_samples(data):\n",
    "    flat_data = data.flatten()\n",
    "    min_val = np.min(flat_data)\n",
    "    max_val = np.max(flat_data)\n",
    "    diff = max_val - min_val\n",
    "    if diff == 0:\n",
    "        diff = 1  # 避免除零错误\n",
    "    normalized_data = (data - min_val) / diff\n",
    "    # 将数据从 [0, 1] 转换到 [-1, 1]\n",
    "    normalized_data = normalized_data * 2 - 1\n",
    "    return normalized_data\n",
    "\n",
    "\n",
    "injection_data = normalize_all_samples(injection_data)\n",
    "print(\"归一化后 injection_data 前五行第一个特征值:\")\n",
    "print(injection_data[:5, 0])\n",
    "\n",
    "pressure_data = normalize_all_samples(pressure_data)\n",
    "print(\"归一化后 pressure_data 前五行第一个特征值:\")\n",
    "print(pressure_data[:5, 0])\n",
    "\n",
    "saturation_data = normalize_all_samples(saturation_data)\n",
    "print(\"归一化后 saturation_data 前五行第一个特征值:\")\n",
    "print(saturation_data[:5, 0])\n",
    "\n",
    "changing_numeric_data = np.stack([\n",
    "    injection_data,\n",
    "    pressure_data,\n",
    "    saturation_data\n",
    "], axis=2)\n",
    "\n",
    "def read_time_series_images(root_dir):\n",
    "    \"\"\"\n",
    "    读取指定根目录下的时间序列图像数据\n",
    "    :param root_dir: 根目录路径\n",
    "    :return: 时间序列图像数据数组\n",
    "    \"\"\"\n",
    "    image_time_series_data = []\n",
    "    for folder in natsorted(os.listdir(root_dir)):\n",
    "        folder_path = os.path.join(root_dir, folder)\n",
    "        if os.path.isdir(folder_path):\n",
    "            time_series_images = []\n",
    "            for img_file in natsorted(os.listdir(folder_path)):\n",
    "                img_path = os.path.join(folder_path, img_file)\n",
    "                print(f\"正在读取文件: {img_path}\")\n",
    "                try:\n",
    "                    img = Image.open(img_path).convert('L')  # 转换为灰度模式\n",
    "                    img = img.resize((image_size, image_size))\n",
    "                    img = np.array(img)\n",
    "                    # 归一化处理到 [0, 1]\n",
    "                    img = img / 255.0\n",
    "                    # 将数据从 [0, 1] 转换到 [-1, 1]\n",
    "                    img = img * 2 - 1\n",
    "                    img = np.expand_dims(img, axis=0)  # 添加通道维度\n",
    "                    time_series_images.append(img)\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to read image: {img_path}, Error: {e}\")\n",
    "\n",
    "            if len(time_series_images) != time_steps:\n",
    "                print(f\"Folder {folder} has {len(time_series_images)} images, expected {time_steps}. Skipping this folder.\")\n",
    "            else:\n",
    "                image_time_series_data.append(time_series_images)\n",
    "\n",
    "    # 将列表转换为NumPy数组并调整维度\n",
    "    image_time_series_data = np.array(image_time_series_data)\n",
    "    # 检查数组维度\n",
    "    if image_time_series_data.ndim != 5:\n",
    "        print(f\"数组维度为 {image_time_series_data.ndim}，不等于 5，无法进行转置。\")\n",
    "        return None\n",
    "    image_time_series_data = np.transpose(image_time_series_data, (0, 1, 2, 3, 4))  # (B, T, C, H, W)\n",
    "    return image_time_series_data\n",
    "\n",
    "\n",
    "# 调用函数读取数据\n",
    "root_dir = r'3000饱和度图片'\n",
    "img_time_series = read_time_series_images(root_dir)\n",
    "\n",
    "# 1.5固定图片数据：渗透率图片、井位图片\n",
    "# 读取图片数据\n",
    "def read_image_data(folder_path, image_size, data_type):\n",
    "    image_files = natsorted(os.listdir(folder_path))\n",
    "    images = []\n",
    "    print(f\"正在读取 {data_type} 图片数据，文件顺序如下：\")\n",
    "    for file in image_files:\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        print(file_path)\n",
    "        try:\n",
    "            # 打开图片并转换为灰度模式\n",
    "            image = Image.open(file_path).convert('L')\n",
    "            # 调整图片大小为指定尺寸\n",
    "            image = image.resize((image_size, image_size))\n",
    "            # 转换为 numpy 数组，并确保数据类型为浮点型\n",
    "            image = np.array(image)\n",
    "            # 归一化处理到 [0, 1]\n",
    "            image = image / 255.0\n",
    "            # 将数据从 [0, 1] 转换到 [-1, 1]\n",
    "            image = image * 2 - 1\n",
    "            images.append(image)\n",
    "        except Exception as e:\n",
    "            print(f\"读取图片 {file_path} 时出错: {e}\")\n",
    "    return np.array(images)\n",
    "\n",
    "\n",
    "# 示例调用\n",
    "permeability_folder = r'3000渗透率图集'\n",
    "well_location_folder = r'3000井位可视化'\n",
    "\n",
    "permeability_images = read_image_data(permeability_folder, image_size, '3000渗透率图集')\n",
    "well_location_images = read_image_data(well_location_folder, image_size, '3000井位可视化')\n",
    "\n",
    "# 调整维度顺序，添加通道维度\n",
    "permeability_images = np.expand_dims(permeability_images, axis=1)\n",
    "well_location_images = np.expand_dims(well_location_images, axis=1)\n",
    "\n",
    "perm_images = torch.from_numpy(permeability_images).float()\n",
    "well_images = torch.from_numpy(well_location_images).float()\n",
    "print(\"转换为张量后渗透率图片数据形状:\", perm_images.shape)\n",
    "print(\"转换为张量后井位图片数据形状:\", well_images.shape)\n",
    "\n",
    "# 合并渗透率图片和井位图片为2通道特征矩阵\n",
    "combined_images = torch.cat([perm_images, well_images], dim=1)\n",
    "print(\"合并后2通道特征矩阵形状:\", combined_images.shape)\n",
    "\n",
    "# 数据转换与检查\n",
    "changing_numeric_data = torch.from_numpy(changing_numeric_data).float()  # (B, T, 3)\n",
    "print(\"转换为张量后可变数值数据形状:\", changing_numeric_data.shape)\n",
    "print(\"转换为张量后可变数值数据前五行第一个时间步的第一个特征值:\")\n",
    "print(changing_numeric_data[:1, :6, 0])\n",
    "\n",
    "img_time_series = torch.from_numpy(img_time_series).float()  # (B, T, C, H, W)\n",
    "print(\"转换为张量后时间序列图像数据形状:\", img_time_series.shape)\n",
    "\n",
    "\n",
    "first_six_rows = changing_numeric_data[:6]\n",
    "print(\"changing_numeric_data 的前 6 行数据:\")\n",
    "print(first_six_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899b2558-d5ae-4a2d-ba8f-3f5e1aee766a",
   "metadata": {},
   "source": [
    "# 二、模型训练+验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4008fb2-d6da-4d83-b43f-2f26a993e46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from PIL import Image\n",
    "from natsort import natsorted\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from tqdm import tqdm\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from pytorch_msssim import SSIM\n",
    "import csv\n",
    "\n",
    "\n",
    "# 残差块\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, 3, stride, 1, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.dropout = nn.Dropout(dropout_rate)  # 添加Dropout \n",
    "        self.relu = nn.ReLU()\n",
    "        self.downsample = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 1, stride, 0, bias=False),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        ) if in_channels != out_channels or stride != 1 else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.conv(x)\n",
    "        out = self.bn(out)\n",
    "        out = self.dropout(out)  # 在激活前应用Dropout\n",
    "        out = self.relu(out)\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(identity)\n",
    "        out = out + identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# 可学习位置编码模块\n",
    "class LearnablePositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=6):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # 定义可学习的位置编码参数（位置数x维度）\n",
    "        self.pos_emb = nn.Parameter(torch.zeros(max_len, d_model))  \n",
    "        nn.init.trunc_normal_(self.pos_emb, std=0.02)  # 正态分布初始化\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, T, D = x.shape  # (batch, time_steps, d_model)\n",
    "        # 生成位置编码并扩展到批次维度\n",
    "        pos_enc = self.pos_emb[:T].unsqueeze(0).repeat(B, 1, 1)  \n",
    "        x = x + pos_enc  # 相加位置编码\n",
    "        return self.dropout(x)\n",
    "\n",
    "# 多模态模型\n",
    "class MultiModalGasModel(nn.Module):\n",
    "    def __init__(self, img_channels=2, time_features=3, fixed_features=5,\n",
    "                 embed_dim=128, nhead=8, img_size=128, time_steps=6):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.time_steps = time_steps\n",
    "\n",
    "        # 空间编码器（添加Dropout）\n",
    "        self.img_encoder = nn.Sequential(\n",
    "            nn.Conv2d(img_channels, 32, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            ResidualBlock(32, 32, dropout_rate=0.1),\n",
    "            nn.Dropout(0.1),  # 额外的Dropout层\n",
    "            nn.Conv2d(32, 64, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            ResidualBlock(64, 64, dropout_rate=0.1),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            ResidualBlock(128, 128, dropout_rate=0.1),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Conv2d(128, embed_dim, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(embed_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # 时间编码器（增强Transformer正则化）\n",
    "        self.time_embed = nn.Linear(time_features, embed_dim)\n",
    "        self.time_pos = LearnablePositionalEncoding(2 * embed_dim, max_len=time_steps)  # 可学习位置编码\n",
    "        self.time_transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=2 * embed_dim,\n",
    "                nhead=nhead,\n",
    "                batch_first=True,\n",
    "                dropout=0.1,  # Transformer内部Dropout\n",
    "                activation='gelu'  # 使用更平滑的激活函数\n",
    "            ),\n",
    "            num_layers=6\n",
    "        )\n",
    "\n",
    "        # 固定编码器\n",
    "        self.fixed_embed = nn.Linear(fixed_features, embed_dim)\n",
    "\n",
    "        # 融合 Transformer\n",
    "        self.fusion_transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=2 * embed_dim,\n",
    "                nhead=nhead,\n",
    "                batch_first=True\n",
    "            ),\n",
    "            num_layers=4\n",
    "        )\n",
    "\n",
    "        # 解码器（添加谱归一化）\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.utils.spectral_norm(  # 谱归一化防止梯度爆炸\n",
    "                nn.ConvTranspose2d(2 * embed_dim, 128, 8, 1, 0, bias=False)\n",
    "            ),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 16, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 1, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, static_img, fixed_num, dynamic_ts):\n",
    "        B, T, F = dynamic_ts.shape\n",
    "        device = fixed_num.device\n",
    "\n",
    "        # 编码静态图像和固定特征（全局特征）\n",
    "        static_feat = self.img_encoder(static_img).flatten(2).transpose(1, 2)  # (B, 64, E)\n",
    "        fixed_feat = self.fixed_embed(fixed_num).unsqueeze(1)  # (B, 1, E)\n",
    "        global_feat = torch.cat([fixed_feat, static_feat], dim=1)  # (B, 65, E)\n",
    "\n",
    "        # 为每个时间步拼接全局特征和动态特征\n",
    "        time_feat = self.time_embed(dynamic_ts)  # (B, T, E)\n",
    "        global_feat_repeated = global_feat.mean(dim=1, keepdim=True).repeat(1, T, 1)  # (B, T, E)\n",
    "        time_feat = torch.cat([global_feat_repeated, time_feat], dim=2)  # (B, T, 2E)\n",
    "        time_feat = self.time_pos(time_feat)\n",
    "        time_encoded = self.time_transformer(time_feat)  # (B, T, 2E)\n",
    "\n",
    "        # 将 global_feat 的维度扩展到 2 * embed_dim\n",
    "        global_feat = torch.cat([global_feat, global_feat], dim=2)  # (B, 65, 2E)\n",
    "\n",
    "        # 融合全局特征和时间编码特征\n",
    "        fused_input = torch.cat([global_feat, time_encoded], dim=1)  # (B, 65 + T, 2E)\n",
    "        fused_feat = self.fusion_transformer(fused_input)  # (B, 65 + T, 2E)\n",
    "\n",
    "        # 提取时间相关的融合特征\n",
    "        time_fused_feat = fused_feat[:, 65:, :]  # (B, T, 2E)\n",
    "\n",
    "        # 解码每个时间步的图像\n",
    "        pred_imgs = []\n",
    "        for t in range(T):\n",
    "            current_feat = time_fused_feat[:, t, :].unsqueeze(1)  # (B, 1, 2E)\n",
    "            pred_img = self.decoder(current_feat.permute(0, 2, 1).unsqueeze(-1))  # 转换为 (B, 2E, 1, 1)\n",
    "            pred_imgs.append(pred_img)\n",
    "\n",
    "        pred_imgs = torch.cat(pred_imgs, dim=1)  # (B, T, 1, 128, 128)\n",
    "        return pred_imgs\n",
    "\n",
    "\n",
    "# 数据集\n",
    "class GasDataset(Dataset):\n",
    "    def __init__(self, static_images, fixed_numeric, dynamic_numeric, target_images):\n",
    "        self.static_images = static_images  # (N, C, H, W)\n",
    "        self.fixed_numeric = fixed_numeric  # (N, F)\n",
    "        self.dynamic_numeric = dynamic_numeric  # (N, T, F)\n",
    "        self.target_images = target_images  # (N, T, C, H, W)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.static_images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            self.static_images[idx],\n",
    "            self.fixed_numeric[idx],\n",
    "            self.dynamic_numeric[idx],\n",
    "            self.target_images[idx]\n",
    "        )\n",
    "\n",
    "\n",
    "def train_epoch(model, loader, optimizer, criterion_mse, criterion_ssim, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_ssim = 0.0\n",
    "    total_mse = 0.0\n",
    "    pbar = tqdm(loader, desc=\"Training\", dynamic_ncols=True)\n",
    "    for batch in pbar:\n",
    "        static_img, fixed_num, dynamic_ts, target_img = [x.to(device).float() for x in batch]  # 转换数据类型\n",
    "        pred_imgs = model(static_img, fixed_num, dynamic_ts)\n",
    "\n",
    "        loss = 0.0\n",
    "        batch_ssim = 0.0\n",
    "        batch_mse = 0.0\n",
    "        for t in range(model.time_steps):\n",
    "            pred = pred_imgs[:, t]\n",
    "            # 调整 pred 的维度，使其和 gt 一致\n",
    "            if pred.dim() == 3:\n",
    "                pred = pred.unsqueeze(1)\n",
    "            gt = target_img[:, t]\n",
    "            loss_mse = criterion_mse(pred, gt)\n",
    "            loss_ssim = 1 - criterion_ssim(pred, gt)\n",
    "            loss += 0.7 * loss_mse + 0.3 * loss_ssim\n",
    "            batch_ssim += criterion_ssim(pred, gt).item()\n",
    "            batch_mse += loss_mse.item()\n",
    "        loss /= model.time_steps\n",
    "        batch_ssim /= model.time_steps\n",
    "        batch_mse /= model.time_steps\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        total_ssim += batch_ssim\n",
    "        total_mse += batch_mse\n",
    "        pbar.set_postfix({\"loss\": total_loss / (pbar.n + 1)})\n",
    "    return total_loss / len(loader), total_ssim / len(loader), total_mse / len(loader)\n",
    "\n",
    "\n",
    "def val_epoch(model, loader, criterion_mse, criterion_ssim, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_ssim = 0.0\n",
    "    total_mse = 0.0\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(loader, desc=\"Validation\", dynamic_ncols=True)\n",
    "        for batch in pbar:\n",
    "            static_img, fixed_num, dynamic_ts, target_img = [x.to(device).float() for x in batch]  # 转换数据类型\n",
    "            pred_imgs = model(static_img, fixed_num, dynamic_ts)\n",
    "\n",
    "            loss = 0.0\n",
    "            batch_ssim = 0.0\n",
    "            batch_mse = 0.0\n",
    "            for t in range(model.time_steps):\n",
    "                pred = pred_imgs[:, t]\n",
    "                # 调整 pred 的维度，使其和 gt 一致\n",
    "                if pred.dim() == 3:\n",
    "                    pred = pred.unsqueeze(1)\n",
    "                gt = target_img[:, t]\n",
    "                loss_mse = criterion_mse(pred, gt)\n",
    "                loss_ssim = 1 - criterion_ssim(pred, gt)\n",
    "                loss += 0.7 * loss_mse + 0.3 * loss_ssim\n",
    "                batch_ssim += criterion_ssim(pred, gt).item()\n",
    "                batch_mse += loss_mse.item()\n",
    "            loss /= model.time_steps\n",
    "            batch_ssim /= model.time_steps\n",
    "            batch_mse /= model.time_steps\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_ssim += batch_ssim\n",
    "            total_mse += batch_mse\n",
    "    return total_loss / len(loader), total_ssim / len(loader), total_mse / len(loader)\n",
    "\n",
    "\n",
    "def save_all_images(model, dataset, device, epoch, dataset_type):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx in tqdm(range(len(dataset)), desc=f\"Saving {dataset_type} images for epoch {epoch}\"):\n",
    "            static_img, fixed_num, dynamic_ts, target_img = dataset[idx]\n",
    "            if isinstance(static_img, np.ndarray):\n",
    "                static_img = torch.from_numpy(static_img).unsqueeze(0).to(device).float()\n",
    "            else:\n",
    "                static_img = static_img.unsqueeze(0).to(device).float()\n",
    "            if isinstance(fixed_num, np.ndarray):\n",
    "                fixed_num = torch.from_numpy(fixed_num).unsqueeze(0).to(device).float()\n",
    "            else:\n",
    "                fixed_num = fixed_num.unsqueeze(0).to(device).float()\n",
    "            if isinstance(dynamic_ts, np.ndarray):\n",
    "                dynamic_ts = torch.from_numpy(dynamic_ts).unsqueeze(0).to(device).float()\n",
    "            else:\n",
    "                dynamic_ts = dynamic_ts.unsqueeze(0).to(device).float()\n",
    "            if isinstance(target_img, np.ndarray):\n",
    "                target_img = torch.from_numpy(target_img).to(device).float()\n",
    "            else:\n",
    "                target_img = target_img.to(device).float()\n",
    "\n",
    "            pred_imgs = model(static_img, fixed_num, dynamic_ts)\n",
    "\n",
    "            for t in range(model.time_steps):\n",
    "                true_img = target_img[t, 0].cpu().numpy()\n",
    "                # 保存真实图片\n",
    "                true_img_path = os.path.join('Result', f'epoch_{epoch}', dataset_type, f'sample_{idx}', 'true_images')\n",
    "                os.makedirs(true_img_path, exist_ok=True)\n",
    "                plt.imsave(\n",
    "                    os.path.join(true_img_path, f'step_{t}.png'),\n",
    "                    (true_img + 1) / 2,\n",
    "                    cmap='viridis'\n",
    "                )\n",
    "\n",
    "                # 保存预测图片\n",
    "                pred = pred_imgs[0, t].cpu().numpy()\n",
    "                if pred.ndim == 3:\n",
    "                    pred = pred.squeeze(0)\n",
    "                pred_img_path = os.path.join('Result', f'epoch_{epoch}', dataset_type, f'sample_{idx}', 'pred_images')\n",
    "                os.makedirs(pred_img_path, exist_ok=True)\n",
    "                plt.imsave(\n",
    "                    os.path.join(pred_img_path, f'step_{t}.png'),\n",
    "                    (pred + 1) / 2,\n",
    "                    cmap='viridis'\n",
    "                )\n",
    "\n",
    "static_images = combined_images  # 静态多通道图像\n",
    "fixed_numeric = fixed_numeric_data  # 固定数值特征\n",
    "dynamic_numeric = changing_numeric_data  # 动态时间序列特征\n",
    "target_images = img_time_series  # 目标图像序列（单通道）\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    torch.autograd.set_detect_anomaly(True)  # 启用异常检测（可选）\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    os.makedirs('Result', exist_ok=True)  # 直接创建主目录\n",
    "\n",
    "    # ================== 修改1：划分训练集、验证集、测试集 ==================\n",
    "    all_indices = np.arange(3000)  # 假设总样本数2000\n",
    "    # 训练集:验证集:测试集 = 6:2:2（先分80%训练+验证，再分其中25%为验证）\n",
    "    train_val_idx, test_idx = train_test_split(all_indices, test_size=0.2, random_state=40)\n",
    "    train_idx, val_idx = train_test_split(train_val_idx, test_size=0.25, random_state=40)  # 0.8*0.25=0.2验证集\n",
    "    \n",
    "    train_dataset = GasDataset(static_images[train_idx], fixed_numeric[train_idx], dynamic_numeric[train_idx],\n",
    "                               target_images[train_idx])\n",
    "    val_dataset = GasDataset(static_images[val_idx], fixed_numeric[val_idx], dynamic_numeric[val_idx],\n",
    "                             target_images[val_idx])\n",
    "    test_dataset = GasDataset(static_images[test_idx], fixed_numeric[test_idx], dynamic_numeric[test_idx],\n",
    "                              target_images[test_idx])  \n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4)  \n",
    "\n",
    "    model = MultiModalGasModel(\n",
    "        img_channels=2,\n",
    "        time_features=3,\n",
    "        fixed_features=5,\n",
    "        embed_dim=128,\n",
    "        img_size=128,\n",
    "        time_steps=6\n",
    "    ).to(device)\n",
    "\n",
    "    criterion_mse = nn.MSELoss()\n",
    "    criterion_ssim = SSIM(data_range=2.0, size_average=True, channel=1)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.0002, weight_decay=3e-5)\n",
    "    scheduler = StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "\n",
    "    # 初始化日志文件（添加测试集相关列）\n",
    "    log_file = os.path.join('Result', 'training_log.csv')\n",
    "    with open(log_file, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['epoch', 'train_loss', 'train_ssim', 'train_mse', \n",
    "                         'val_loss', 'val_ssim', 'val_mse', 'test_loss', 'test_ssim', 'test_mse'])  # 新增测试列\n",
    "\n",
    "    num_epochs = 50\n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_ssim, train_mse = train_epoch(model, train_loader, optimizer, criterion_mse, criterion_ssim, device)\n",
    "        val_loss, val_ssim, val_mse = val_epoch(model, val_loader, criterion_mse, criterion_ssim, device)\n",
    "        scheduler.step()\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train SSIM: {train_ssim:.4f}, Train MSE: {train_mse:.4f}, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val SSIM: {val_ssim:.4f}, Val MSE: {val_mse:.4f}\")\n",
    "\n",
    "\n",
    "        with open(log_file, 'a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([epoch + 1, train_loss, train_ssim, train_mse, \n",
    "                             val_loss, val_ssim, val_mse, '-', '-', '-'])  # 训练时测试列留空\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), os.path.join('Result', 'best_model.pth'))\n",
    "\n",
    "        # 保存指定epoch的训练/验证图片\n",
    "        save_epochs = [50]\n",
    "        if (epoch + 1) in save_epochs:\n",
    "            save_all_images(model, train_dataset, device, epoch + 1, 'train')\n",
    "            save_all_images(model, val_dataset, device, epoch + 1, 'val')\n",
    "\n",
    "    # ================== 修改2：训练后评估测试集 ==================\n",
    "    print(\"\\n================= 测试集评估 ==================\")\n",
    "    # 加载最佳模型\n",
    "    model.load_state_dict(torch.load(os.path.join('Result', 'best_model.pth')))\n",
    "    model.eval()\n",
    "    \n",
    "    # 评估测试集\n",
    "    test_loss, test_ssim, test_mse = val_epoch(model, test_loader, criterion_mse, criterion_ssim, device)\n",
    "    print(f\"Test SSIM: {test_ssim:.4f}  \\tTest MSE: {test_mse:.4f}  \\tTest Loss: {test_loss:.4f}\")\n",
    "    \n",
    "    # 保存测试集图片（使用最后epoch编号或特殊标识）\n",
    "    save_all_images(model, test_dataset, device, num_epochs, 'test')\n",
    "    \n",
    "    # 补充测试集日志（可选）\n",
    "    with open(log_file, 'a', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['Test', '-', '-', '-', '-', '-', '-', test_loss, test_ssim, test_mse])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
